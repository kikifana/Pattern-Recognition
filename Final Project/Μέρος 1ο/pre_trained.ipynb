{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lB2jUQKk8h1"
      },
      "source": [
        "# 1. Μέρος 1ο – Ταξινόμηση με Συνελικτικά Νευρωνικά Δίκτυα (60%)\n",
        "### d. Πειραματιστείτε και προτείνετε ένα CNN δικής σας έμπνευσης που να βελτιστοποιεί την ακρίβεια ταξινόμησης στο test set χωρίς ενδείξεις overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJyvyalwk-Il"
      },
      "source": [
        "Για λόγους σύγκρισης πραγματοποιήθηκε και pre trained CNN που δείχνει πόσο γρήγορα και καλά αποδίδουν, καθώς έχουν ήδη γίνει trained στην ImageNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6TEeJ5TG7wj"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import SGD,Adam,lr_scheduler\n",
        "from torch.utils.data import random_split\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRuG8B4ElUCo"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([transforms.Resize(size=(224, 224)),\n",
        "                                      transforms.RandomHorizontalFlip(p = 0.4), # FLips the image w.r.t horizontal axis\n",
        "                                      transforms.RandomRotation(30), #Rotates the image to a specified angel\n",
        "                                      transforms.Pad(4),\n",
        "                                      transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)), #Performs actions like zooms, change shear angles.\n",
        "                                      transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Set the color params\n",
        "                                      transforms.ToTensor(), # comvert the image to tensor so that it can work with torch\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) #Normalize all the images\n",
        "                               ])\n",
        " \n",
        " \n",
        "test_transform = transforms.Compose([transforms.Resize(size=(224, 224)),\n",
        "                               transforms.Pad(4),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                               ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Jq3-Mf4laK5",
        "outputId": "26341daa-1c1b-46b5-f9ba-d9a4509a9458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "train_data = datasets.CIFAR10(root='data', train=True, download=True, transform=train_transform)\n",
        "test_data = datasets.CIFAR10(root='data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "trainloader = DataLoader(dataset=train_data, \n",
        "                          batch_size=40,\n",
        "                          num_workers=2,\n",
        "                          shuffle=True)\n",
        "\n",
        "testloader = DataLoader(dataset=test_data, \n",
        "                         batch_size=40,\n",
        "                         num_workers=2,\n",
        "                         shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3MGWv6fldx3"
      },
      "outputs": [],
      "source": [
        "from torchvision import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njwhdDV1l5Hr",
        "outputId": "bb27d8e9-0387-4ea9-b782-0800fb2dc9c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "model = models.vgg16(pretrained = True)\n",
        "input_lastLayer = model.classifier[6].in_features\n",
        "model.classifier[6] = nn.Linear(input_lastLayer,10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQVbZljmmKpJ",
        "outputId": "145875db-4c8c-48d1-d8d1-30384588afeb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC0a2BU2u9dE"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.001, momentum=0.9,weight_decay=5e-4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "epochs = 4"
      ],
      "metadata": {
        "id": "ae0h8yQPO2X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for epoch in range(epochs):\n",
        "   for i, (imgs , labels) in enumerate(trainloader):\n",
        "       imgs = imgs.to(device)\n",
        "       labels = labels.to(device)\n",
        "\n",
        "       labels_hat = model(imgs)\n",
        "       n_corrects = (labels_hat.argmax(axis=1)==labels).sum().item()\n",
        "       loss_value = criterion(labels_hat, labels)\n",
        "       loss_value.backward()\n",
        "       optimizer.step()\n",
        "       optimizer.zero_grad()\n",
        "       if (i+1) % 250 == 0:\n",
        "           print(f'epoch {epoch+1}/{epochs}, step: {i+1}/{len(trainloader)}: loss = {loss_value:.5f}, acc = {100*(n_corrects/labels.size(0)):.2f}%')\n",
        "   print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeaM6wVgaNOu",
        "outputId": "9f24b66c-f2d2-4b53-a1eb-149f0aaf43ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1/4, step: 250/1250: loss = 0.83605, acc = 60.00%\n",
            "epoch 1/4, step: 500/1250: loss = 0.65351, acc = 75.00%\n",
            "epoch 1/4, step: 750/1250: loss = 0.49315, acc = 82.50%\n",
            "epoch 1/4, step: 1000/1250: loss = 0.52730, acc = 85.00%\n",
            "epoch 1/4, step: 1250/1250: loss = 0.30114, acc = 85.00%\n",
            "\n",
            "epoch 2/4, step: 250/1250: loss = 0.57333, acc = 80.00%\n",
            "epoch 2/4, step: 500/1250: loss = 0.36119, acc = 85.00%\n",
            "epoch 2/4, step: 750/1250: loss = 0.48034, acc = 85.00%\n",
            "epoch 2/4, step: 1000/1250: loss = 0.11929, acc = 97.50%\n",
            "epoch 2/4, step: 1250/1250: loss = 0.24122, acc = 92.50%\n",
            "\n",
            "epoch 3/4, step: 250/1250: loss = 0.29677, acc = 90.00%\n",
            "epoch 3/4, step: 500/1250: loss = 0.47425, acc = 82.50%\n",
            "epoch 3/4, step: 750/1250: loss = 0.45379, acc = 82.50%\n",
            "epoch 3/4, step: 1000/1250: loss = 0.31214, acc = 87.50%\n",
            "epoch 3/4, step: 1250/1250: loss = 0.36431, acc = 85.00%\n",
            "\n",
            "epoch 4/4, step: 250/1250: loss = 0.40915, acc = 85.00%\n",
            "epoch 4/4, step: 500/1250: loss = 0.33769, acc = 85.00%\n",
            "epoch 4/4, step: 750/1250: loss = 0.26209, acc = 90.00%\n",
            "epoch 4/4, step: 1000/1250: loss = 0.49693, acc = 85.00%\n",
            "epoch 4/4, step: 1250/1250: loss = 0.11573, acc = 97.50%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Παρατηρούμε, όπως είναι αναμενόμενο, πως το pre trained δίκτυο αποδίδει πολύ καλύτερα σε πολύ λιγότερο χρόνο. "
      ],
      "metadata": {
        "id": "ed5Vi_clpPji"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}